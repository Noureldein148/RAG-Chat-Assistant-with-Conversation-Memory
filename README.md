# RAG-Chat-Assistant-with-Conversation-Memory

ğŸ§  RAG Chat Assistant with Conversation Memory

This project is a Retrieval-Augmented Generation (RAG) chatbot that answers questions based on document content while maintaining conversation history. The application uses embeddings and vector search to retrieve relevant document chunks and combines them with chat memory to generate context-aware responses.

ğŸš€ Features

ğŸ“„ PDF document knowledge retrieval

ğŸ’¬ Chat history memory (context-aware conversations)

ğŸ” Semantic search using embeddings

ğŸ¤– LLM-powered answers (Groq Llama Model)

ğŸŒ Streamlit web interface

ğŸ§  LangChain RAG pipeline

ğŸ› ï¸ Tech Stack

Python

Streamlit

LangChain

Chroma Vector Database

OpenAI Embeddings

Groq LLM (Llama 3.3 70B)

âš™ï¸ How It Works

Load PDF document

Split text into chunks

Generate embeddings

Store in vector database

Retrieve relevant context

Combine with chat history

Generate LLM response

ğŸ“‚ Use Cases

Document Q&A

Research assistants

Knowledge base chatbots

Study helpers
